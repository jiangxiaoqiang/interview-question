\documentclass[../../../interview-questions.tex]{subfiles}

\begin{document}

\subsection{\color{red}{epoll(Event Poll)原理解析}}

\paragraph{epoll介绍}

epoll 又是一种 I/O 复用的机制。它的功能与 poll 类似，都是监听多个文件描述符上的IO事件。 所不同的是 epoll 接口有边沿触发(edge-triggered)和电平触发(level-triggered)两种形式，适合监听超大规模的文件描述符。 当 epoll 工作在电平触发的形式下时，其实现的功能与 poll 是等价的，只是获取事件的方式略有差异。

epoll 的接口稍微多了一点，需要先使用 epoll\_create 来构造 epoll 实例，然后通过 epoll\_ctl 注册事件，最后调用 epoll\_wait 接口监听事件。 调用 epoll\_wait 时，线程会进入阻塞的状态，直到内核捕获到了事件。epoll 接口的核心就是这个通过 epoll\_create 构造出来的 epoll 实例。 从我们用户的角度来看，epoll 实例就是关注(interest)和就绪(ready)两个列表。我们通过 epoll\_ctl 向关注列表中添加需要监听的文件描述符和事件。 系统内核会根据实际的 IO 活动更新就绪列表，记录下那些准备就绪的 IO 操作，通过 epoll\_wait 告知用户，用户只需要检查这个就绪列表就可以了。 不必像 poll 那样遍历一遍所有的文件描述符。

epoll 接口最主要的一个特性就是，它有边沿触发(edge-triggered)和电平触发(level-triggered)两种形式。 这里我特意将 edge 和 level 翻译成边沿和电平，如果读者做过嵌入式的开发，相信对这两个名词再熟悉不过了。 如右图所示，假设有一个数字信号将要从 0 切换到 1 再切换回 0。在电路上，我们通常用不同的低/高电压来表示这个0/1逻辑。 如果有示波器可以显示出电压的变化波形，低/高电压就是两个水平的线，被称为低/高电平。而状态发生变化的一瞬间会有一个陡峭的边沿， 从0变到1就是一个上升的过程，称为上升沿，从1变到0就是一个下降的过程，称为下降沿。那么所谓的边沿触发， 就是在状态发生变化的时候触发一次。而电平触发则是只要信号仍然是高电平就不停的通知用户。

比如说，我们在一个 epoll 实例中监听了一个文件描述符(fd)的可读事件。我们把输入缓存中是否有数据看做是一个数字信号， 0表示没有数据可读，1表示有数据可读。一开始输入缓存是空的，然后我们以某种手段在这个缓存中写入了2k字节的数据，此时无论是边沿触发还是电平触发， 都会导致 epoll\_wait 返回。因为信号发生了变化，并且此时还是高电平。返回之后，通过 fd 我们只读走了1k字节的数据，就再次调用了 epoll\_wait。 那么在边沿触发的工作模式下，程序会再次阻塞，因为状态并没有发生变化，不存在上升沿。 而电平触发的工作模式下，会立即返回，因为此时仍然是高电平。

epoll机制是Java NIO的地基，epoll是Netty实现高并发的基础，epoll是Redis实现高性能的秘诀，epoll是Nginx实现高性能的核心，epoll是Go协程的秘诀，所以理解epoll是非常有必要的。

\paragraph{epoll的演进}

在epoll之前，Linux内核提供了select、poll、epoll。以 select 和 epoll 来对比举例，select池子里管理了 1024 个句柄，为什么只有1024个句柄？因为select会遍历句柄列表，为了性能考量，限制最大为1024。loop 线程被唤醒的时候，select 都是蒙的，都不知道这 1024 个 fd 里谁 IO 准备好了。这种情况怎么办？只能遍历这 1024 个 fd ，一个个测试。假如只有一个句柄准备好了，那相当于做了 1 千多倍的无效功，所以效率很低下。由于 select 存在上面的问题，于是 poll 被提了出来，poll本质上和select没有区别，只是它没有最大连接数的限制，原因是它是基于链表来存储的，它将用户传入的需要监视的文件描述符拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。它能解决 select 对文件描述符数量有限制的问题，但是依然不能解决线性遍历以及用户空间和内核空间的低效数据拷贝问题。select/poll 在互联网早期应该是没什么问题的，因为没有很多的互联网服务，也没有很多的客户端，但是随着互联网的发展，C10K 等问题的出现，select/poll 已经不能满足要求了，这个时候 epoll 上场了。epoll 是 linux 内核 2.6 之后支持的，epoll 同 select/poll 一样，也是 IO 多路复用的一种机制，不过它避免了 select/poll 的缺点。epoll 则不同，从 epoll\_wait 醒来的时候就能精确的拿到就绪的 fd 数组，不需要任何测试，拿到的就是要处理的。epoll的通俗解释是一种当文件描述符的内核缓冲区非空的时候，发出可读信号进行通知，当写缓冲区不满的时候，发出可写信号通知的机制。IO 多路复用的原始实现很简单，就是一个 1 对多的服务模式，一个 loop 对应处理多个 fd ；
IO 多路复用想要做到真正的高效，必须要内核机制提供。因为 IO 的处理和完成是在内核，如果内核不帮忙，用户态的程序根本无法精确的抓到处理时机；
fd 记得要设置成非阻塞的哦，切记；
epoll 池通过高效的内部管理结构，并且结合操作系统提供的 poll 事件注册机制，实现了高效的 fd 事件管理，为高并发的 IO 处理提供了前提条件；
epoll 全名 eventpoll，在 Linux 内核下以一个文件系统模块的形式实现，所以有人常说 epoll 其实本身就是文件系统也是对的；
socketfd，eventfd，timerfd 这三种”文件“fd 实现了 poll 接口，所以网络 fd，事件fd，定时器fd 都可以使用 epoll\_ctl 注册到池子里。我们最常见的就是网络fd的多路复用；

\begin{enumerate}
    \item {如何突破文件描述符数量的限制}
    \item {如何避免用户态和内核态对文件描述符集合的拷贝}
    \item {socket 就绪后，如何避免线性遍历文件描述符集合}
\end{enumerate}

针对第一点：如何突破文件描述符(File Descriptor)数量的限制，其实 poll 已经解决了，poll 使用的是链表的方式管理 socket 描述符，但问题是不够高效，如果有百万级别的连接需要管理，如何快速的插入和删除就变得很重要，于是 epoll 采用了红黑树的方式进行管理，这样能保证在添加 socket 和删除 socket 时，有 O(log(n)) 的复杂度\footnote{内容来源：\url{https://rebootcat.com/2020/09/26/epoll_cookbook/}}。

针对第二点：如何避免用户态和内核态对文件描述符集合的拷贝，其实对于 select 来说，由于这个集合是保存在用户态的，所以当调用 select 时需要屡次的把这个描述符集合拷贝到内核空间。所以如果要解决这个问题，可以直接把这个集合放在内核空间进行管理。没错，epoll 就是这样做的，epoll 在内核空间创建了一颗红黑树，应用程序直接把需要监控的 socket 对象添加到这棵树上，直接从用户态到内核态了，而且后续也不需要再次拷贝了。

针对第三点：socket就绪后，如何避免内核线性遍历文件描述符集合，这个问题就会比较复杂，要完整理解就得涉及到内核收包到应用层的整个过程。这里先简单讲一下，与 select 不同，epoll 使用了一个双向链表来保存就绪的 socket，这样当活跃连接数不多的情况下，应用程序只需要遍历这个就绪链表就行了，而 select 没有这样一个用来存储就绪 socket 的东西，导致每次需要线性遍历所有socket，以确定是哪个或者哪几个 socket 就绪了。这里需要注意的是，这个就绪链表保存活跃链接，数量是较少的，也需要从内核空间拷贝到用户空间。

\paragraph{系统调用}

epollcreate 负责创建一个池子，一个监控和管理句柄 fd 的池子；
epollctl 负责管理这个池子里的 fd 增、删、改；
epollwait 就是负责打盹的，让出 CPU 调度，但是只要有“事”，立马会从这里唤醒；

\paragraph{epoll高效原理}





\end{document}